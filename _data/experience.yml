# Jobs
# Amino Inc
- company: <a href="https://hypertrack.com">Hypertrack Inc</a>
  position: Lead Data Scientist
  duration: August, 2018 &mdash; February, 2019
  summary: I worked on data pipelines and algorithms to make accurate and descriptive stories out of workforce movement. I wrote about <a href="http://blog.hypertrack.com/blog/2018/10/04/reducing-on-demand-delivery-time-with-activity-data/">Reducing on-demand delivery time with activity data</a>. Some of my projects,
  list: <ul class="resume-item-list"><li>Places of interest identification - Designed and implemented a highly scalable pipeline to process GPS data from 5000+ devices and identify places of interest such as home and work for each device using scala, elasticsearch, cron utils, docker and kubernetes</li><li>Transit Classification - designed and implemented an algorithm to classify route taken by device into mode of transportation - subway, car, flight, ferry</li><li>Delivery Trip Activity Segmentation - designed and implemented an algorithm to segment delivery trip into pickup, parking, walking, dropping off and delivery</li>
  details:
# Amino Inc
- company: <a href="https://amino.com">Amino Inc</a>
  position: Data Scientist
  duration: April, 2016 &mdash; August, 2018
  summary: I worked on algorithms and data pipelines for the cost transparency tool. I lead development of multiple machine learning models in a 3-4 data science team. I was the technical lead on a project partnership between Amino and <a href="https://en.wikipedia.org/wiki/Aon_(company)">Aon</a>. I collaborated with senior actuaries and consultants who have had extensive experience in the health benefits space. I have written about my work on <a href="https://www.linkedin.com/pulse/demystifying-healthcare-costs-canonical-episodes-care-shekhar/">Demystifying healthcare costs with canonical episodes of care</a> and <a href="https://amino.com/blog/how-deep-learning-can-help-us-understand-physician-specialties-from-billions-of-insurance-claims/">How deep learning can help us understand physician specialties from billions of insurance claims</a>. Some of my projects,
  list: <ul class="resume-item-list"><li>Facility Cost Ratings - Designed and implemented the pipeline to calculate a expensiveness rating on a scale of 0 to 1. For thousands of medical facilities, each from a robust regression analysis of their price data across all services</li>
  details:
# Lumiata Inc
- company: <a href="https://lumiata.com">Lumiata</a>
  position: Lead Data Engineer
  duration:  July, 2014 &mdash; March, 2016
  summary: I lead multiple projects in a team of 5-7 data engineers. I worked as a lead engineer on the design and implementation of the claims data pipeline. It ran graph algorithms on billions of claim files ingested from ftp, fhir server or REST API. We built this on a <a href="https://mesosphere.com/blog/smack-stack-new-lamp-stack/">SMACK stack</a>. Some of my projects,
  list:  <ul class="resume-item-list"><li>HCC Coding and Reimbursement Model - Implemented a parser in scala which extracted decision rules from csvs and excel files provided online by Dept. of Human Health Services to implement a crosswalk from patient information to a coding scheme (HCC) used to estimate annual expenditure on medicare enrollees</li><li>Drug Profiles - A data mining project to design a knowledge graph on RXNORM entities with data fields such as dosing, related diagnosis, side effects and code morbities as attributes within a drug node from multiple online sources. I worked in collaboration with a pharmacist, used python for web scraping and elastic search as the database.</li></ul>